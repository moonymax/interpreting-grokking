{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2478c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dc3b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.amp.grad_scaler import GradScaler\n",
    "import math\n",
    "\n",
    "\n",
    "# Initialize GradScaler for BFloat16 training\n",
    "scaler = GradScaler(enabled=True)\n",
    "\n",
    "\n",
    "class LongRope(nn.Module):\n",
    "    \"\"\"\n",
    "    LongRope (Rotary Position Embeddings) implementation for extending context window.\n",
    "    BFloat16 compatible version using sin/cos tables instead of complex numbers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, base=10000.0, scaling_factor=0.25, max_seq_len=16384):\n",
    "        \"\"\"\n",
    "        Initialize the LongRope module.\n",
    "        \n",
    "        Args:\n",
    "            dim (int): The embedding dimension (must be even)\n",
    "            base (float): Base value for frequency calculations, default is 10000.0\n",
    "            scaling_factor (float): Scaling factor to extend context window\n",
    "            max_seq_len (int): Maximum sequence length to pre-compute\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        if dim % 2 != 0:\n",
    "            raise ValueError(f\"Dimension must be even, got {dim}\")\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Pre-compute sin and cos tables instead of complex numbers for BFloat16 compatibility\n",
    "        self.register_buffer('cos_cached', None)\n",
    "        self.register_buffer('sin_cached', None)\n",
    "        self._precompute_freqs()\n",
    "        \n",
    "    def _precompute_freqs(self):\n",
    "        \"\"\"Precompute sin and cos tables for rotary embeddings\"\"\"\n",
    "        # Compute frequencies with scaling\n",
    "        theta = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n",
    "        theta = theta * self.scaling_factor\n",
    "        \n",
    "        # Pre-compute tables for positions up to max_seq_len\n",
    "        position = torch.arange(self.max_seq_len).float()\n",
    "        freqs = torch.outer(position, theta)\n",
    "        \n",
    "        # Cache sin and cos values instead of complex numbers\n",
    "        self.register_buffer('cos_cached', torch.cos(freqs).float())\n",
    "        self.register_buffer('sin_cached', torch.sin(freqs).float())\n",
    "    \n",
    "    def _rotate_half(self, x):\n",
    "        \"\"\"Rotate half the hidden dims of x\"\"\"\n",
    "        x1, x2 = x.chunk(2, dim=-1)\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "    \n",
    "    def _apply_rotary_pos_emb(self, x, seq_len):\n",
    "        \"\"\"\n",
    "        Apply rotary position embeddings using separate sin and cos tables\n",
    "        Compatible with BFloat16\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_len, dim]\n",
    "            seq_len (int): Sequence length\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Tensor with rotary embeddings applied\n",
    "        \"\"\"\n",
    "        # Get the appropriate part of the cached tables\n",
    "        cos = self.cos_cached[:seq_len].unsqueeze(0)  # [1, seq_len, dim/2]\n",
    "        sin = self.sin_cached[:seq_len].unsqueeze(0)  # [1, seq_len, dim/2]\n",
    "        \n",
    "        # Make sure cos and sin have the right dimensions for broadcasting\n",
    "        # We need to repeat each value twice to match the original dimensions\n",
    "        cos = torch.repeat_interleave(cos, 2, dim=-1)  # [1, seq_len, dim]\n",
    "        sin = torch.repeat_interleave(sin, 2, dim=-1)  # [1, seq_len, dim]\n",
    "        \n",
    "        # Apply rotation using the trigonometric addition formulas\n",
    "        # This is equivalent to complex multiplication but works with any dtype\n",
    "        return x * cos + self._rotate_half(x) * sin\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply rotary position embeddings to input tensor.\"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        if seq_len > self.max_seq_len:\n",
    "            raise ValueError(f\"Input sequence length {seq_len} exceeds maximum sequence length {self.max_seq_len}\")\n",
    "        \n",
    "        return self._apply_rotary_pos_emb(x, seq_len)\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1, rope_scaling_factor=0.25, max_seq_len=4096):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        # Create query, key, value projections and output layer\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize LongRope for each head dimension\n",
    "        self.rope = LongRope(\n",
    "            dim=self.head_dim,\n",
    "            scaling_factor=rope_scaling_factor,\n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Project query, key, value and split into multiple heads\n",
    "        q = self.query(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Apply LongRope to queries and keys for each head\n",
    "        # First, reshape for applying rope to each head separately\n",
    "        q_reshaped = q.reshape(batch_size * self.n_heads, seq_len, self.head_dim)\n",
    "        k_reshaped = k.reshape(batch_size * self.n_heads, seq_len, self.head_dim)\n",
    "        \n",
    "        # Apply LongRope positional embeddings\n",
    "        q_rope = self.rope(q_reshaped).view(batch_size, self.n_heads, seq_len, self.head_dim)\n",
    "        k_rope = self.rope(k_reshaped).view(batch_size, self.n_heads, seq_len, self.head_dim)\n",
    "        \n",
    "        # Compute attention scores using the rotary-embedded queries and keys\n",
    "        scores = torch.matmul(q_rope, k_rope.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Create causal (lower triangular) mask to prevent attending to future positions\n",
    "        device = x.device\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()\n",
    "        scores.masked_fill_(causal_mask, -1e9)\n",
    "        \n",
    "        # Apply softmax and dropout\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention weights to values (no need to apply RoPE to values)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Reshape output\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        return self.out(attn_output)\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1, rope_scaling_factor=0.25, max_seq_len=4096):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Self-attention with LongRope\n",
    "        self.self_attn = CausalSelfAttention(\n",
    "            d_model=d_model, \n",
    "            n_heads=n_heads, \n",
    "            dropout=dropout,\n",
    "            rope_scaling_factor=rope_scaling_factor,\n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization and dropout\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply self-attention with residual connection and layer norm\n",
    "        attn_output = self.self_attn(self.norm1(x))\n",
    "        x = x + self.dropout(attn_output)\n",
    "        \n",
    "        # Apply feed-forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(self.norm2(x))\n",
    "        x = x + self.dropout(ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 d_model=512, \n",
    "                 n_heads=8, \n",
    "                 n_layers=6, \n",
    "                 d_ff=2048, \n",
    "                 dropout=0.1,\n",
    "                 rope_scaling_factor=0.25,\n",
    "                 max_seq_len=4096):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # No separate positional encoding since we're using RoPE in the attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(\n",
    "                d_model=d_model, \n",
    "                n_heads=n_heads, \n",
    "                d_ff=d_ff, \n",
    "                dropout=dropout,\n",
    "                rope_scaling_factor=rope_scaling_factor,\n",
    "                max_seq_len=max_seq_len\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "        \n",
    "    def _init_parameters(self):\n",
    "        # Initialize parameters with Xavier/Glorot initialization\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Get token embeddings\n",
    "        x = self.token_embedding(x)\n",
    "        \n",
    "        # Apply dropout to embeddings\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Apply decoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        # Apply final layer norm and output projection\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Define training function with BFloat16 mixed precision\n",
    "def train_transformer(model, dataloader, optimizer, criterion, device, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            # Prepare input and target sequences\n",
    "            input_ids = batch[:, :-1].to(device)  # all tokens except last\n",
    "            target_ids = batch[:, 1:].to(device)  # all tokens except first\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with autocast for BFloat16\n",
    "            with autocast(dtype=torch.bfloat16):\n",
    "                logits = model(input_ids)\n",
    "                \n",
    "                # Reshape for cross-entropy loss\n",
    "                logits = logits.view(-1, logits.size(-1))\n",
    "                targets = target_ids.reshape(-1)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(logits, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "261dfc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class ArithmeticTokenizer:\n",
    "    \"\"\"\n",
    "    A simple tokenizer that operates on arithmetic symbols (numbers, space, +, =).\n",
    "    Supports character-level tokenization, special tokens, and can handle unknown characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define special tokens and their IDs\n",
    "        self.pad_token = \"[PAD]\"\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.bos_token = \"[BOS]\"\n",
    "        self.eos_token = \"[EOS]\"\n",
    "        \n",
    "        # Create character to ID mapping\n",
    "        self.char_to_id = {}\n",
    "        self.id_to_char = {}\n",
    "        \n",
    "        # Add special tokens\n",
    "        self.special_tokens = {\n",
    "            self.pad_token: 0,\n",
    "            self.unk_token: 1,\n",
    "            self.bos_token: 2,\n",
    "            self.eos_token: 3\n",
    "        }\n",
    "        \n",
    "        # Add only the necessary characters for arithmetic (digits, space, +, =)\n",
    "        # Add digits 0-9\n",
    "        for i in range(10):\n",
    "            char = str(i)\n",
    "            token_id = i + len(self.special_tokens)\n",
    "            self.char_to_id[char] = token_id\n",
    "            self.id_to_char[token_id] = char\n",
    "        \n",
    "        # Add space, plus, and equals sign\n",
    "        arithmetic_chars = [\"+\", \"=\"]\n",
    "        for char in arithmetic_chars:\n",
    "            token_id = len(self.char_to_id) + len(self.special_tokens)\n",
    "            self.char_to_id[char] = token_id\n",
    "            self.id_to_char[token_id] = char\n",
    "            \n",
    "        # Add special tokens to id_to_char mapping\n",
    "        for token, id_ in self.special_tokens.items():\n",
    "            self.id_to_char[id_] = token\n",
    "        \n",
    "        # Total vocabulary size\n",
    "        self.vocab_size = len(self.char_to_id) + len(self.special_tokens)\n",
    "    \n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        \"\"\"\n",
    "        Encode a text string into a list of token IDs.\n",
    "        \n",
    "        Args:\n",
    "            text: The input text to tokenize\n",
    "            add_special_tokens: Whether to add BOS/EOS tokens\n",
    "            \n",
    "        Returns:\n",
    "            A list of token IDs\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        tokens = []\n",
    "        \n",
    "        # Add BOS token if requested\n",
    "        if add_special_tokens:\n",
    "            tokens.append(self.special_tokens[self.bos_token])\n",
    "        \n",
    "        # Convert characters to token IDs\n",
    "        for char in text:\n",
    "            if char in self.char_to_id:\n",
    "                tokens.append(self.char_to_id[char])\n",
    "            else:\n",
    "                tokens.append(self.special_tokens[self.unk_token])\n",
    "        \n",
    "        # Add EOS token if requested\n",
    "        if add_special_tokens:\n",
    "            tokens.append(self.special_tokens[self.eos_token])\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into a text string.\n",
    "        \n",
    "        Args:\n",
    "            token_ids: List of token IDs to decode\n",
    "            skip_special_tokens: Whether to skip special tokens in output\n",
    "            \n",
    "        Returns:\n",
    "            The decoded text\n",
    "        \"\"\"\n",
    "        chars = []\n",
    "        \n",
    "        for token_id in token_ids:\n",
    "            # Check if token ID exists\n",
    "            if token_id not in self.id_to_char:\n",
    "                continue\n",
    "                \n",
    "            token = self.id_to_char[token_id]\n",
    "            \n",
    "            # Skip special tokens if requested\n",
    "            if skip_special_tokens and token in self.special_tokens:\n",
    "                continue\n",
    "                \n",
    "            chars.append(token)\n",
    "        \n",
    "        return ''.join(chars)\n",
    "    \n",
    "    def encode_batch(self, texts, add_special_tokens=True, padding=True, return_tensors=None):\n",
    "        \"\"\"\n",
    "        Encode a batch of texts.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to encode\n",
    "            add_special_tokens: Whether to add BOS/EOS tokens\n",
    "            padding: Whether to pad sequences to the same length\n",
    "            return_tensors: Return PyTorch tensors if 'pt'\n",
    "            \n",
    "        Returns:\n",
    "            List of token ID lists or padded tensor\n",
    "        \"\"\"\n",
    "        encoded_texts = [self.encode(text, add_special_tokens) for text in texts]\n",
    "        \n",
    "        if padding:\n",
    "            # Find maximum length\n",
    "            max_len = max(len(encoded) for encoded in encoded_texts)\n",
    "            \n",
    "            # Pad sequences\n",
    "            padded_texts = []\n",
    "            for encoded in encoded_texts:\n",
    "                padding_length = max_len - len(encoded)\n",
    "                padded = encoded + [self.special_tokens[self.pad_token]] * padding_length\n",
    "                padded_texts.append(padded)\n",
    "            \n",
    "            encoded_texts = padded_texts\n",
    "        \n",
    "        if return_tensors == 'pt':\n",
    "            import torch\n",
    "            return torch.tensor(encoded_texts)\n",
    "        \n",
    "        return encoded_texts\n",
    "    \n",
    "    def decode_batch(self, token_ids_batch, skip_special_tokens=True):\n",
    "        \"\"\"\n",
    "        Decode a batch of token ID lists back into text strings.\n",
    "        \n",
    "        Args:\n",
    "            token_ids_batch: Batch of token ID lists or 2D tensor\n",
    "            skip_special_tokens: Whether to skip special tokens in output\n",
    "            \n",
    "        Returns:\n",
    "            List of decoded text strings\n",
    "        \"\"\"\n",
    "        if hasattr(token_ids_batch, 'tolist'):  # Check if it's a tensor-like object\n",
    "            token_ids_batch = token_ids_batch.tolist()\n",
    "            \n",
    "        return [self.decode(token_ids, skip_special_tokens) for token_ids in token_ids_batch]\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        \"\"\"\n",
    "        Returns the vocabulary as a dictionary of token to token ID.\n",
    "        \"\"\"\n",
    "        vocab = self.char_to_id.copy()\n",
    "        vocab.update(self.special_tokens)\n",
    "        return vocab\n",
    "    \n",
    "    def get_special_tokens_mask(self, token_ids):\n",
    "        \"\"\"\n",
    "        Creates a mask for special tokens in a sequence.\n",
    "        \n",
    "        Args:\n",
    "            token_ids: List of token IDs\n",
    "            \n",
    "        Returns:\n",
    "            A boolean mask with True at positions of special tokens\n",
    "        \"\"\"\n",
    "        special_token_ids = set(self.special_tokens.values())\n",
    "        return [token_id in special_token_ids for token_id in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ce04051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98347/441969060.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Create a BFloat16 GradScaler - initialized once here\n",
    "scaler = GradScaler(enabled=True)\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=128):\n",
    "        self.tokenized_texts = []\n",
    "        \n",
    "        for text in texts:\n",
    "            # Tokenize text with our ASCII tokenizer\n",
    "            tokens = tokenizer.encode(text)\n",
    "            # if len(tokens) > max_len:\n",
    "            #     tokens = tokens[:max_len]\n",
    "            self.tokenized_texts.append(torch.tensor(tokens, dtype=torch.long))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_texts[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Pad all sequences to the max length in this batch\n",
    "    max_len = max(len(x) for x in batch)\n",
    "    padded_batch = []\n",
    "    \n",
    "    for sequence in batch:\n",
    "        # Create padded sequence\n",
    "        padded_seq = torch.zeros(max_len, dtype=torch.long)\n",
    "        padded_seq[:len(sequence)] = sequence\n",
    "        padded_batch.append(padded_seq)\n",
    "    \n",
    "    return torch.stack(padded_batch)\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        # Move batch to device\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # Prepare input and target sequences for language modeling\n",
    "        input_ids = batch[:, :-1]  # all tokens except last\n",
    "        target_ids = batch[:, 1:]  # all tokens except first\n",
    "        \n",
    "        # Forward pass with BFloat16 mixed precision\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Use autocast with correct parameters for your PyTorch version\n",
    "        with autocast(dtype=torch.bfloat16):\n",
    "            logits = model(input_ids)\n",
    "        \n",
    "            # Reshape for cross-entropy loss\n",
    "            logits = logits.reshape(-1, logits.size(-1))\n",
    "            targets = target_ids.reshape(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, targets)\n",
    "        \n",
    "        # Backward pass with gradient scaling for BFloat16\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            # Move batch to device\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Prepare input and target sequences\n",
    "            input_ids = batch[:, :-1]\n",
    "            target_ids = batch[:, 1:]\n",
    "            \n",
    "            # Forward pass with BFloat16 mixed precision\n",
    "            with autocast(dtype=torch.bfloat16):\n",
    "                logits = model(input_ids)\n",
    "                \n",
    "                # Reshape for cross-entropy loss\n",
    "                logits = logits.reshape(-1, logits.size(-1))\n",
    "                targets = target_ids.reshape(-1)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(logits, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# Full training function with BFloat16\n",
    "def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, device, \n",
    "                num_epochs=10, patience=3, model_path='best_model.pt'):\n",
    "    # Check if BFloat16 is supported\n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        print(\"Training with BFloat16 mixed precision\")\n",
    "    else:\n",
    "        print(\"Warning: BFloat16 not supported on this GPU. Using FP32 instead.\")\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, criterion, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss = validate(model, val_dataloader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "            }, model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs without improvement\")\n",
    "                break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8d94f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=0.8, top_k=0, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode the prompt BUT REMOVE the EOS token if present\n",
    "    prompt_tokens = tokenizer.encode(prompt)\n",
    "    \n",
    "    # Check if the last token is EOS and remove it\n",
    "    eos_token_id = tokenizer.special_tokens[tokenizer.eos_token]\n",
    "    if prompt_tokens[-1] == eos_token_id:\n",
    "        # print(\"Removing EOS token from prompt tokens\")\n",
    "        prompt_tokens = prompt_tokens[:-1]\n",
    "    \n",
    "    # Convert to tensor and move to device\n",
    "    input_ids = torch.tensor(prompt_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate tokens as before\n",
    "    generated_token_ids = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step in range(max_length):\n",
    "            # Get model predictions with BFloat16 mixed precision\n",
    "            with autocast(dtype=torch.bfloat16):\n",
    "                outputs = model(input_ids)\n",
    "                next_token_logits = outputs[:, -1, :] / temperature\n",
    "            \n",
    "            # Apply top-k filtering if specified\n",
    "            if top_k > 0:\n",
    "                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
    "                mask = torch.zeros_like(next_token_logits).scatter_(1, top_k_indices, 1)\n",
    "                next_token_logits = next_token_logits.masked_fill(mask == 0, -float('inf'))\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            next_token_id = next_token.item()\n",
    "            \n",
    "            # Store just the new token\n",
    "            generated_token_ids.append(next_token_id)\n",
    "            \n",
    "            # Append the next token to input for the next iteration\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "            \n",
    "            # Stop if we generate an EOS token\n",
    "            if next_token_id == eos_token_id:\n",
    "                # print(f\"Generated EOS token at step {step}, stopping generation\")\n",
    "                break\n",
    "    \n",
    "    # Decode JUST the newly generated tokens\n",
    "    generated_text = tokenizer.decode(generated_token_ids)\n",
    "    \n",
    "    # For verification, show the full sequence too\n",
    "    full_sequence = prompt_tokens + generated_token_ids\n",
    "    full_text = tokenizer.decode(full_sequence)\n",
    "    \n",
    "    # print(f\"Full text: '{full_text}'\")\n",
    "    \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa567b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8249320000+2986720000=0236150000\n"
     ]
    }
   ],
   "source": [
    "# # generate data\n",
    "# import random\n",
    "\n",
    "# def generate_sample():\n",
    "#     a = random.randint(0, 100_000_000)\n",
    "#     b = random.randint(0, 100_000_000)\n",
    "#     c = a + b\n",
    "#\n",
    "    # # reverse the string because it might lead to better generalization because of token by token generation\n",
    "    # return f\"{str(a)[::-1].ljust(10, \"0\")}+{str(b)[::-1].ljust(10, \"0\")}={str(c)[::-1].ljust(10, \"0\")}\"\n",
    "\n",
    "# samples = [generate_sample() for i in range(1_000_000)]\n",
    "# print(samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab717e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8249320000+2986720000=0236150000\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# print(samples[0])\n",
    "\n",
    "# with open(\"data/addition_samples_0_to_100_000_000_num_samples_1_000_000_inverted_no_spaces_leftpad_10digits.json\", \"w\") as file:\n",
    "#     json.dump(samples, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/addition_samples_0_to_100_000_000_num_samples_1_000_000_inverted_no_spaces_leftpad_10digits.json\", \"r\") as file:\n",
    "    samples = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "aabe29c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "batch_size = 128\n",
    "# Create tokenizer\n",
    "tokenizer = ArithmeticTokenizer()\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Sample training data\n",
    "# In a real scenario, you would load your corpus from files\n",
    "sample_texts = samples\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TextDataset(sample_texts, tokenizer)\n",
    "\n",
    "# Split into train/validation sets\n",
    "train_size = int(0.95 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "579defb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98347/3832636820.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "BFloat16 is supported on this device\n",
      "Loaded optimizer state from checkpoint\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/7422 [00:00<?, ?it/s]/tmp/ipykernel_98347/441969060.py:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=torch.bfloat16):\n",
      "Training: 100%|██████████| 7422/7422 [00:26<00:00, 284.12it/s]\n",
      "Validation:   0%|          | 0/391 [00:00<?, ?it/s]/tmp/ipykernel_98347/441969060.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=torch.bfloat16):\n",
      "Validation: 100%|██████████| 391/391 [00:00<00:00, 716.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Train Loss: 0.8487 | Val Loss: 0.8375 | Time: 26.67s\n",
      "Saved new best model!\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize gradient scaler for BFloat16 mixed precision\n",
    "scaler = GradScaler(enabled=True)\n",
    "\n",
    "# Parameters\n",
    "d_model = 64\n",
    "n_heads = 16\n",
    "n_layers = 1\n",
    "d_ff = 256\n",
    "# max_seq_len = 32\n",
    "epochs = 1\n",
    "lr = 3e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_path = \"models/d64_h16_n1_ff256_continuous_training_reversed_and_padded_numbers.pt\"\n",
    "\n",
    "# Check if BFloat16 is supported\n",
    "if hasattr(torch.cuda, 'is_bf16_supported') and torch.cuda.is_bf16_supported():\n",
    "    print(\"BFloat16 is supported on this device\")\n",
    "else:\n",
    "    print(\"WARNING: BFloat16 may not be fully supported on this device.\")\n",
    "\n",
    "# First create your model\n",
    "model = DecoderOnlyTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    d_ff=d_ff,\n",
    "    # max_seq_len=max_seq_len,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "if os.path.isfile(model_path):\n",
    "    # Load the checkpoint dictionary\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move model to device - don't convert to BFloat16 here\n",
    "# We'll handle that with autocast\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# If loading from checkpoint, also load optimizer state\n",
    "if os.path.isfile(model_path) and 'optimizer_state_dict' in checkpoint:\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(\"Loaded optimizer state from checkpoint\")\n",
    "\n",
    "# Ignore padding token (ID 0) in loss calculation\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.special_tokens[tokenizer.pad_token])\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, criterion, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(model, val_dataloader, criterion, device)\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} | \"\n",
    "            f\"Time: {elapsed:.2f}s\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "        }, model_path)\n",
    "        print(\"Saved new best model!\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "860ddf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation examples:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGeneration examples:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m test_prompts:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     generated = \u001b[43mgenerate_text\u001b[49m(\n\u001b[32m     13\u001b[39m         model,\n\u001b[32m     14\u001b[39m         tokenizer,\n\u001b[32m     15\u001b[39m         prompt,\n\u001b[32m     16\u001b[39m         max_length=\u001b[32m50\u001b[39m,\n\u001b[32m     17\u001b[39m         temperature=\u001b[32m0.8\u001b[39m,\n\u001b[32m     18\u001b[39m         top_k=\u001b[32m5\u001b[39m,\n\u001b[32m     19\u001b[39m         device=device\n\u001b[32m     20\u001b[39m     )\n\u001b[32m     21\u001b[39m     prompt_reversed = \u001b[33m\"\u001b[39m\u001b[33m+\u001b[39m\u001b[33m\"\u001b[39m.join([substr[::-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m substr \u001b[38;5;129;01min\u001b[39;00m prompt.replace(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).split(\u001b[33m\"\u001b[39m\u001b[33m+\u001b[39m\u001b[33m\"\u001b[39m)]) + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m     generated_reversed = generated[::-\u001b[32m1\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_text' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test generation\n",
    "test_prompts = [\n",
    "    \"9999999900+1000000000=\",\n",
    "    \"0200000000+0100000000=\",\n",
    "    \"1000010000+1000000000=\",\n",
    "    \"2000000000+2000000000=\",\n",
    "    \"0010000000+0020000000=\"\n",
    "]\n",
    "\n",
    "print(\"\\nGeneration examples:\")\n",
    "for prompt in test_prompts:\n",
    "    generated = generate_text(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        temperature=0.8,\n",
    "        top_k=5,\n",
    "        device=device\n",
    "    )\n",
    "    prompt_reversed = \"+\".join([substr[::-1] for substr in prompt.replace(\"=\", \"\").split(\"+\")]) + \"=\"\n",
    "    generated_reversed = generated[::-1]\n",
    "    print(f\"Prompt: '{prompt_reversed}'\")\n",
    "    print(f\"Generated: '{generated_reversed}'\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
